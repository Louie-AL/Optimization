{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"lfxhYxHsbGWE"},"source":["# **Recitation 12**"]},{"cell_type":"markdown","metadata":{"id":"3KWVWgZGbNRR"},"source":["# Reference Code\n","Consider the following reference code for gradient descent on logistic regression from homework."]},{"cell_type":"code","metadata":{"id":"X4jBrQ4NbEJs"},"source":["# Import packages\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pqTt7ZHFbpyh"},"source":["# Generate Logistic Regression Data\n","\n","# Constants\n","tau = 1 # Standard deviation of each class\n","n   = 40  # Examples per class\n","trunc = 5 # Number of initial steps to remove while plotting\n","\n","# Generate red and green centers in [0,1]x[0,1] box\n","np.random.seed(1)\n","r = np.array([1,0])\n","g = np.array([-1,0])\n","\n","# Generate red and green points\n","rpoints = (np.random.randn(n,2) * tau) + r\n","gpoints = (np.random.randn(n,2) * tau) + g\n","\n","# Create data\n","X = np.vstack([rpoints, gpoints])\n","Y = np.append(np.ones(n), np.zeros(n))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_yh5R_DRb0LW"},"source":["# Gradient Descent Code\n","\n","# Gradient Desent Parameters\n","w0  = np.array([1,1]) # Initial w\n","t0  = 1     # Initial step-size\n","eps = 1e-5  # Stoping criteria\n","\n","# sigmoid function\n","def sig(z):\n","  return 1/(1+np.exp(-z))\n","sig = np.vectorize(sig) # Allows us to apply this to entire arrays\n","\n","# *negative* log-likelihood (since we are maximizing)\n","def nll(w):\n","  # Inputs to the sigmoid.\n","  wX = w @ X.T\n","\n","  # Vector of log probabilities\n","  # Note: \"*\" is element-wise multiplication in Python\n","  logP = Y * np.log(sig(wX)) + (1 - Y) * np.log(sig(-wX))\n","  return -sum(logP)\n","\n","# gradient of *negative* log-likelihood (since we are maximizing)\n","def gradnll(w):\n","  # Inputs to the sigmoid.\n","  wX = w @ X.T\n","\n","  # Calculate the gradient\n","  # Note = \"axis=0\" means \"sum the rows\"\n","  return -np.sum((Y - sig(wX))[:,np.newaxis] * X, axis=0)\n","\n","# Define function that performs one iteration of gradient descent, with linesearch\n","def GDiter(w):\n","  obj = nll(w) # objective\n","  grad = gradnll(w) # gradient\n","  grad_norm2 = np.linalg.norm(grad)**2 # squared norm of gradient\n","\n","  done = False # Have not completed linesearch\n","  t = t0 # Set initial step size\n","\n","  oracle_calls = 0\n","  # Perform line search\n","  while not done:\n","    w_new = w - t * grad\n","\n","    oracle_calls += 1\n","    if nll(w_new) > obj - (t/2) * grad_norm2: # If there is enough decrease\n","      t = 0.5 * t # Shrink step-size by 0.5\n","    else:\n","      done = True\n","\n","  # Return new point\n","  return w_new, oracle_calls\n","\n","# Perform gradient descent\n","i = 0 # Counts iterations\n","w = w0\n","grad_norm = np.linalg.norm(gradnll(w))\n","\n","# Run gradient descent\n","i = 0\n","oracle_calls = 0\n","while grad_norm > eps: # Stopping condition\n","  i += 1\n","  w, calls = GDiter(w)\n","  oracle_calls += calls # Count oracle calls\n","\n","  grad_norm = np.linalg.norm(gradnll(w))\n","  obj = nll(w)\n","\n","  # Print progress\n","  print('iter = %s; oracle_calls = %s; w = %s; neg-loglik=%.4g; ||grad|| = %.3g'%(i, oracle_calls, w, obj, grad_norm))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9ZXOQQKWc6ia"},"source":["# **(a)**\n","In the line-search described by the lecture-notes, if the condition $f\\left(x^{+}\\right)>f(x)-\\frac{t}{2}\\|\\nabla f(x)\\|^{2}$ is not met, then we decrease the step-size by a shrikage factor $1/\\gamma$. $gamma=2$ is most commonly used, but the $0.5$-factor shrinkage is not sacred. You can also use other factors.\n","\n","Do the following:\n","\n","1. Copy-and-paste the reference code.\n","2. Identify the part where the line-search is done.\n","3. Change the shrinkage-factor to 0.25.\n","4. Rerun the code.\n","5. Comment on what changes you see in behavior of gradient descent.\n","6. Repeat steps 1-5 with a shrinkage factor of 0.75.\n"]},{"cell_type":"code","metadata":{"id":"MLBFL8hweVCC"},"source":["## ADD CODE FOR PART (a) BELOW ##\n","\n","# 0.25-shrink version"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fqax3BVBekwj"},"source":["# 0.75-shrink version"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"34qSVRYre9cW"},"source":["# **(b)**\n","\n","**Background:** In this problem, we will compute the \"analytic center\" of a polyhedron. In particular, suppose we have some vectors $a_1,\\dots,a_m \\in \\mathbb{R}^n$. From the lectures on linear programs, we know that the following inequalities will create a polyhedral region in $n$-dimensional space:\n","\n","$$a_{i}^{T} x \\leq 1, \\quad i=1, \\ldots, m, \\quad\\left|x_{i}\\right| \\leq 1, \\quad i=1, \\ldots, n$$\n","\n","Solving for the \"analytic center\" mathematically formalizes the task of finding a \"point in the middle\" of this polyhedron. This is done by solving the optimization problem below. For concreteness, we will consider $n=5$ and $m=3$ in this problem.\n","\n","**Problem:** Given three vectors $a_j \\in \\mathbb{R}^{5}$ for $j=1,2,3$.\n","Consider the below, consider the objective function with inputs $x \\in \\mathbb{R}^{5}$, where we denote $i$-th entry of $x$ with $x_i$. The minimizer of this objective function is the \"analytic center\" of $a_j$ for $j=1,2,3$.\n","$$f(x) = -\\sum_{i=1}^{5} \\ln(1-x_i^2) - \\sum_{j=1}^{3}\\ln(1 - a_j^T x).$$\n","Note that the objective is not-every defined. In particular, if $(1-x_i^2)\\leq 0$ for any $i$ or if $(1-a_j^T x) \\leq 0$ for any $j$, the logarithms are not defined.\n","\n","Assume our initial point $x_0$ does not run into this issue. There is a way to modify the line-search to prevent future steps from becoming undefined as well.\n","\n","Specifically, the linesearch from the lecture notes changes to the following:\n","\n","**IF** $(f\\left(x^{+}\\right)>f(x)-\\frac{t}{2}\\|\\nabla f(x)\\|^{2})$ **OR** (There is $i$ where $(1-x_i^2)\\leq 0$) **OR** (There is $j$ where $(1-a_j^T x)\\leq 0$):  \n","\n","  $\\quad t = 0.5t$\n","\n","**ELSE**\n","\n","  $\\quad \\texttt{done}$\n","\n","\n","Do the following:\n","\n","1. Generate an initial iterate $x_0 = (0,\\dots,0)$, i.e. the length-5 vector with all zeros, using `x0=np.zeros(5)`.\n","2. Set the random seed using `np.random.seed(0)`.\n","3. Generate the three $a_j$ randomly (i.e. for $j = 1,2,3$) using the `np.random.randn(5)` command.\n","4. Implement gradient descent on the new $f(x)$ with the line-search described above.\n","\n","  Hint 1: You could use the reference code as a template.\n","\n","  Hint 2: The *partial derivative* of $f$ w.r.t. each $x_i$ is $$ \\frac{\\partial f}{\\partial x_i}(x) = \\frac{2 x_i}{1-x_i^2} + \\sum_{j=1}^3 \\frac{a_{ji}}{1-a_j^T x},$$\n","  where $a_{ji}$ is the $i$-th entry of the vector $a_j$. Use this fact to create the gradient $\\nabla f(x)$.\n","3. Run the code."]},{"cell_type":"code","metadata":{"id":"gypyjtgJike0"},"source":["## ADD CODE FOR PART (b) BELOW ##"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tu1J2MQYlEyd"},"source":["# **(c)**\n","\n","Do the following:\n","1. Copy-and-paste your code from part (b).\n","2. Change the step-size shrinkage to 0.25.\n","3. Run the code.\n","4. Comment on what differences you see in behavior of the algorithm.\n","5. Repeat steps 1-4 using a step-size shrinkage of 0.75."]},{"cell_type":"code","metadata":{"id":"DuYYZyRoECkP"},"source":["## ADD CODE FOR PART (c) BELOW ##\n","\n","# 0.25-shrink version"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EIVnHaScEwlT"},"source":["# 0.75-factor version"],"execution_count":null,"outputs":[]}]}